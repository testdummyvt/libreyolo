{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LibreYOLO8 - Minimal Inference Notebook\n",
        "\n",
        "This notebook contains all the code needed to run inference with a self-contained implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from pathlib import Path\n",
        "from typing import Union, Tuple, Dict, List\n",
        "import colorsys\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DFL(nn.Module):\n",
        "    \"\"\"Distribution Focal Loss (DFL) module.\"\"\"\n",
        "    def __init__(self, c1=16):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)\n",
        "        x = torch.arange(c1, dtype=torch.float)\n",
        "        self.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1))\n",
        "        self.c1 = c1\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        x = x.view(b, 4, self.c1, h, w).transpose(2, 1)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        x = self.conv(x.reshape(b, self.c1, 4 * h, w))\n",
        "        return x.view(b, 4, h, w)\n",
        "\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    \"\"\"Standard convolution: Conv2d + BatchNorm + SiLU\"\"\"\n",
        "    def __init__(self, c_out, c_in, k, s, p):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Conv2d(in_channels=c_in, out_channels=c_out, kernel_size=k, stride=s, padding=p, bias=False)\n",
        "        self.batchnorm = nn.BatchNorm2d(num_features=c_out)\n",
        "        self.silu = nn.SiLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.silu(self.batchnorm(self.cnn(x)))\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    \"\"\"Residual bottleneck block\"\"\"\n",
        "    def __init__(self, c_out, c_in, res_connection):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv(c_out=c_out, c_in=c_in, k=3, s=1, p=1)\n",
        "        self.conv2 = Conv(c_out=c_out, c_in=c_out, k=3, s=1, p=1)\n",
        "        self.res_connection = res_connection\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.res_connection:\n",
        "            return x + self.conv2(self.conv1(x))\n",
        "        return self.conv2(self.conv1(x))\n",
        "\n",
        "\n",
        "class C2F(nn.Module):\n",
        "    \"\"\"C2f module with split-concat-bottleneck structure\"\"\"\n",
        "    def __init__(self, c_out, c_in, res_connection, nb_bottlenecks):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv(c_out=c_out, c_in=c_in, k=1, s=1, p=0)\n",
        "        self.conv2 = Conv(c_out=c_out, c_in=int((nb_bottlenecks + 2) * c_out / 2), k=1, s=1, p=0)\n",
        "        self.bottlenecks = nn.ModuleList([\n",
        "            Bottleneck(c_out=c_out//2, c_in=c_out//2, res_connection=res_connection) \n",
        "            for _ in range(nb_bottlenecks)\n",
        "        ])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        batch, c, h, w = x.shape\n",
        "        x2 = x[:, c//2:, :, :]\n",
        "        for bottleneck in self.bottlenecks:\n",
        "            x2 = bottleneck(x2)\n",
        "            x = torch.cat((x, x2), dim=1)\n",
        "        return self.conv2(x)\n",
        "\n",
        "\n",
        "class SPPF(nn.Module):\n",
        "    \"\"\"Spatial Pyramid Pooling Fast\"\"\"\n",
        "    def __init__(self, c_out, c_in):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv(c_out=c_out//2, c_in=c_in, k=1, s=1, p=0)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)\n",
        "        self.conv2 = Conv(c_out=c_out, c_in=4*(c_out//2), k=1, s=1, p=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x1 = self.maxpool(x)\n",
        "        x2 = self.maxpool(x1)\n",
        "        x3 = self.maxpool(x2)\n",
        "        return self.conv2(torch.cat((x, x1, x2, x3), dim=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Backbone(nn.Module):\n",
        "    \"\"\"Feature extraction backbone\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        cfg = {\n",
        "            'n': {'d': 0.33, 'w': 0.25, 'r': 2.0},\n",
        "            's': {'d': 0.33, 'w': 0.50, 'r': 2.0},\n",
        "            'm': {'d': 0.67, 'w': 0.75, 'r': 1.5},\n",
        "            'l': {'d': 1.00, 'w': 1.00, 'r': 1.0},\n",
        "            'x': {'d': 1.00, 'w': 1.25, 'r': 1.0}\n",
        "        }\n",
        "        d, w, r = cfg[config]['d'], cfg[config]['w'], cfg[config]['r']\n",
        "                \n",
        "        self.p1 = Conv(c_out=int(64*w), c_in=3, k=3, s=2, p=1)\n",
        "        self.p2 = Conv(c_out=int(128*w), c_in=int(64*w), k=3, s=2, p=1)\n",
        "        self.c2f1 = C2F(c_out=int(128*w), c_in=int(128*w), res_connection=True, nb_bottlenecks=max(1, int(round(3*d))))\n",
        "        self.p3 = Conv(c_out=int(256*w), c_in=int(128*w), k=3, s=2, p=1)\n",
        "        self.c2f2 = C2F(c_out=int(256*w), c_in=int(256*w), res_connection=True, nb_bottlenecks=max(1, int(round(6*d))))\n",
        "        self.p4 = Conv(c_out=int(512*w), c_in=int(256*w), k=3, s=2, p=1)\n",
        "        self.c2f3 = C2F(c_out=int(512*w), c_in=int(512*w), res_connection=True, nb_bottlenecks=max(1, int(round(6*d))))\n",
        "        self.p5 = Conv(c_out=int(512*w*r), c_in=int(512*w), k=3, s=2, p=1)\n",
        "        self.c2f4 = C2F(c_out=int(512*w*r), c_in=int(512*w*r), res_connection=True, nb_bottlenecks=max(1, int(round(3*d))))\n",
        "        self.sppf = SPPF(c_out=int(512*w*r), c_in=int(512*w*r))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.c2f1(self.p2(self.p1(x)))\n",
        "        x = self.c2f2(self.p3(x))\n",
        "        x8 = x.clone()\n",
        "        x = self.c2f3(self.p4(x))\n",
        "        x16 = x.clone()\n",
        "        x32 = self.sppf(self.c2f4(self.p5(x)))\n",
        "        return x8, x16, x32\n",
        "\n",
        "\n",
        "class Neck(nn.Module):\n",
        "    \"\"\"Feature pyramid network neck\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        cfg = {\n",
        "            'n': {'d': 0.33, 'w': 0.25, 'r': 2.0},\n",
        "            's': {'d': 0.33, 'w': 0.50, 'r': 2.0},\n",
        "            'm': {'d': 0.67, 'w': 0.75, 'r': 1.5},\n",
        "            'l': {'d': 1.00, 'w': 1.00, 'r': 1.0},\n",
        "            'x': {'d': 1.00, 'w': 1.25, 'r': 1.0}\n",
        "        }\n",
        "        d, w, r = cfg[config]['d'], cfg[config]['w'], cfg[config]['r']\n",
        "\n",
        "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')        \n",
        "        self.c2f21 = C2F(c_out=int(512*w), c_in=int(512*w*(1 + r)), res_connection=False, nb_bottlenecks=max(1, int(round(3*d))))\n",
        "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.c2f11 = C2F(c_out=int(256*w), c_in=int(768*w), res_connection=False, nb_bottlenecks=max(1, int(round(3*d))))    \n",
        "        self.conv1 = Conv(c_out=int(256*w), c_in=int(256*w), k=3, s=2, p=1)\n",
        "        self.c2f12 = C2F(c_out=int(512*w), c_in=int(768*w), res_connection=False, nb_bottlenecks=max(1, int(round(3*d))))\n",
        "        self.conv2 = Conv(c_out=int(512*w), c_in=int(512*w), k=3, s=2, p=1)\n",
        "        self.c2f22 = C2F(c_out=int(512*w*r), c_in=int(512*w*(1 + r)), res_connection=False, nb_bottlenecks=max(1, int(round(3*d))))\n",
        "        \n",
        "    def forward(self, x8, x16, x32):\n",
        "        x16 = self.c2f21(torch.cat((self.upsample1(x32), x16), dim=1))\n",
        "        x8 = self.c2f11(torch.cat((self.upsample2(x16), x8), dim=1))\n",
        "        x16 = self.c2f12(torch.cat((self.conv1(x8), x16), dim=1))\n",
        "        x32 = self.c2f22(torch.cat((self.conv2(x16), x32), dim=1))\n",
        "        return x8, x16, x32\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"Decoupled detection head\"\"\"\n",
        "    def __init__(self, c_in, c_box, c_cls, reg_max, nb_classes):\n",
        "        super().__init__()\n",
        "        self.conv11 = Conv(c_out=c_box, c_in=c_in, k=3, s=1, p=1)\n",
        "        self.conv12 = Conv(c_out=c_box, c_in=c_box, k=3, s=1, p=1)\n",
        "        self.conv21 = Conv(c_out=c_cls, c_in=c_in, k=3, s=1, p=1)\n",
        "        self.conv22 = Conv(c_out=c_cls, c_in=c_cls, k=3, s=1, p=1)\n",
        "        self.cnn1 = nn.Conv2d(in_channels=c_box, out_channels=4*reg_max, kernel_size=1, stride=1, padding=0)\n",
        "        self.cnn2 = nn.Conv2d(in_channels=c_cls, out_channels=nb_classes, kernel_size=1, stride=1, padding=0)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        box = self.cnn1(self.conv12(self.conv11(x)))\n",
        "        cls = self.cnn2(self.conv22(self.conv21(x)))\n",
        "        return box, cls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LibreYOLO8Model(nn.Module):\n",
        "    \"\"\"Main Libre YOLO model\"\"\"\n",
        "    def __init__(self, config, reg_max=16, nb_classes=80):\n",
        "        super().__init__()\n",
        "        cfg = {\n",
        "            'n': {'d': 0.33, 'w': 0.25, 'r': 2.0},\n",
        "            's': {'d': 0.33, 'w': 0.50, 'r': 2.0},\n",
        "            'm': {'d': 0.67, 'w': 0.75, 'r': 1.5},\n",
        "            'l': {'d': 1.00, 'w': 1.00, 'r': 1.0},\n",
        "            'x': {'d': 1.00, 'w': 1.25, 'r': 1.0}\n",
        "        }\n",
        "        w, r = cfg[config]['w'], cfg[config]['r']\n",
        "        \n",
        "        self.backbone = Backbone(config=config)\n",
        "        self.neck = Neck(config=config)\n",
        "        \n",
        "        c_p3 = int(256 * w)\n",
        "        c_box = max(64, c_p3 // 4)\n",
        "        c_cls = max(c_p3, min(nb_classes, 100))\n",
        "        \n",
        "        self.head8 = Head(c_in=int(256*w), c_box=c_box, c_cls=c_cls, reg_max=reg_max, nb_classes=nb_classes)\n",
        "        self.head16 = Head(c_in=int(512*w), c_box=c_box, c_cls=c_cls, reg_max=reg_max, nb_classes=nb_classes)\n",
        "        self.head32 = Head(c_in=int(512*w*r), c_box=c_box, c_cls=c_cls, reg_max=reg_max, nb_classes=nb_classes)\n",
        "        self.dfl = DFL(c1=reg_max)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x8, x16, x32 = self.backbone(x)\n",
        "        x8, x16, x32 = self.neck(x8, x16, x32)\n",
        "        \n",
        "        box8, cls8 = self.head8(x8)\n",
        "        box16, cls16 = self.head16(x16)\n",
        "        box32, cls32 = self.head32(x32)\n",
        "        \n",
        "        return {\n",
        "            'x8': {'box': self.dfl(box8), 'cls': cls8},\n",
        "            'x16': {'box': self.dfl(box16), 'cls': cls16},\n",
        "            'x32': {'box': self.dfl(box32), 'cls': cls32}\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COCO_CLASSES = [\n",
        "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
        "    'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',\n",
        "    'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
        "    'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
        "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
        "    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
        "    'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',\n",
        "    'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
        "    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
        "]\n",
        "\n",
        "\n",
        "def get_class_color(class_id: int) -> str:\n",
        "    \"\"\"Get a unique color for a class ID.\"\"\"\n",
        "    hue = (class_id * 137.508) % 360 / 360.0\n",
        "    saturation = 0.7 + (class_id % 3) * 0.1\n",
        "    value = 0.8 + (class_id % 2) * 0.15\n",
        "    rgb = colorsys.hsv_to_rgb(hue, saturation, value)\n",
        "    return f\"#{int(rgb[0]*255):02x}{int(rgb[1]*255):02x}{int(rgb[2]*255):02x}\"\n",
        "\n",
        "\n",
        "def preprocess_image(image: Union[str, Image.Image, np.ndarray], input_size: int = 640):\n",
        "    \"\"\"Preprocess image for model inference.\"\"\"\n",
        "    if isinstance(image, str):\n",
        "        img = Image.open(image).convert('RGB')\n",
        "    elif isinstance(image, Image.Image):\n",
        "        img = image.convert('RGB')\n",
        "    elif isinstance(image, np.ndarray):\n",
        "        img = Image.fromarray(image).convert('RGB')\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported image type: {type(image)}\")\n",
        "    \n",
        "    original_size = img.size\n",
        "    original_img = img.copy()\n",
        "    img_resized = img.resize((input_size, input_size), Image.Resampling.BILINEAR)\n",
        "    img_array = np.array(img_resized, dtype=np.float32) / 255.0\n",
        "    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)\n",
        "    \n",
        "    return img_tensor, original_img, original_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_anchors(feats: List[torch.Tensor], strides: List[int], grid_cell_offset: float = 0.5):\n",
        "    \"\"\"Generate anchor points from feature map sizes.\"\"\"\n",
        "    anchor_points, stride_tensor = [], []\n",
        "    \n",
        "    for feat, stride in zip(feats, strides):\n",
        "        _, _, h, w = feat.shape\n",
        "        dtype, device = feat.dtype, feat.device\n",
        "        sx = torch.arange(end=w, device=device, dtype=dtype) + grid_cell_offset\n",
        "        sy = torch.arange(end=h, device=device, dtype=dtype) + grid_cell_offset\n",
        "        sy, sx = torch.meshgrid(sy, sx, indexing='ij')\n",
        "        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
        "        stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n",
        "    \n",
        "    return torch.cat(anchor_points), torch.cat(stride_tensor)\n",
        "\n",
        "\n",
        "def decode_boxes(box_preds: torch.Tensor, anchors: torch.Tensor, stride_tensor: torch.Tensor):\n",
        "    \"\"\"Decode box predictions to xyxy coordinates.\"\"\"\n",
        "    x1 = (anchors[:, 0:1] - box_preds[0, :, 0:1]) * stride_tensor[:, 0:1]\n",
        "    y1 = (anchors[:, 1:2] - box_preds[0, :, 1:2]) * stride_tensor[:, 0:1]\n",
        "    x2 = (anchors[:, 0:1] + box_preds[0, :, 2:3]) * stride_tensor[:, 0:1]\n",
        "    y2 = (anchors[:, 1:2] + box_preds[0, :, 3:4]) * stride_tensor[:, 0:1]\n",
        "    return torch.cat([x1, y1, x2, y2], dim=1)\n",
        "\n",
        "\n",
        "def nms(boxes: torch.Tensor, scores: torch.Tensor, iou_threshold: float = 0.45):\n",
        "    \"\"\"Non-Maximum Suppression.\"\"\"\n",
        "    if len(boxes) == 0:\n",
        "        return torch.tensor([], dtype=torch.long, device=boxes.device)\n",
        "    \n",
        "    _, order = scores.sort(0, descending=True)\n",
        "    keep = []\n",
        "    \n",
        "    while len(order) > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i.item())\n",
        "        if len(order) == 1:\n",
        "            break\n",
        "        \n",
        "        box_i = boxes[i]\n",
        "        boxes_remaining = boxes[order[1:]]\n",
        "        \n",
        "        x1_i, y1_i, x2_i, y2_i = box_i\n",
        "        x1_r, y1_r, x2_r, y2_r = boxes_remaining[:, 0], boxes_remaining[:, 1], boxes_remaining[:, 2], boxes_remaining[:, 3]\n",
        "        \n",
        "        x1_inter = torch.max(x1_i, x1_r)\n",
        "        y1_inter = torch.max(y1_i, y1_r)\n",
        "        x2_inter = torch.min(x2_i, x2_r)\n",
        "        y2_inter = torch.min(y2_i, y2_r)\n",
        "        \n",
        "        inter_area = torch.clamp(x2_inter - x1_inter, min=0) * torch.clamp(y2_inter - y1_inter, min=0)\n",
        "        area_i = (x2_i - x1_i) * (y2_i - y1_i)\n",
        "        area_r = (x2_r - x1_r) * (y2_r - y1_r)\n",
        "        iou = inter_area / (area_i + area_r - inter_area + 1e-7)\n",
        "        \n",
        "        order = order[1:][iou < iou_threshold]\n",
        "    \n",
        "    return torch.tensor(keep, dtype=torch.long, device=boxes.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def postprocess(output: Dict, conf_thres: float = 0.25, iou_thres: float = 0.45, \n",
        "                input_size: int = 640, original_size: Tuple[int, int] = None):\n",
        "    \"\"\"Postprocess model outputs to get final detections.\"\"\"\n",
        "    box_layers = [output['x8']['box'], output['x16']['box'], output['x32']['box']]\n",
        "    cls_layers = [output['x8']['cls'], output['x16']['cls'], output['x32']['cls']]\n",
        "    strides = [8, 16, 32]\n",
        "    \n",
        "    anchors, stride_tensor = make_anchors(box_layers, strides)\n",
        "    box_preds = torch.cat([x.flatten(2).permute(0, 2, 1) for x in box_layers], dim=1)\n",
        "    cls_preds = torch.cat([x.flatten(2).permute(0, 2, 1) for x in cls_layers], dim=1)\n",
        "    \n",
        "    decoded_boxes = decode_boxes(box_preds, anchors, stride_tensor)\n",
        "    scores = cls_preds[0].sigmoid()\n",
        "    max_scores, class_ids = torch.max(scores, dim=1)\n",
        "    \n",
        "    mask = max_scores > conf_thres\n",
        "    if not mask.any():\n",
        "        return {\"boxes\": [], \"scores\": [], \"classes\": [], \"num_detections\": 0}\n",
        "    \n",
        "    valid_boxes = decoded_boxes[mask]\n",
        "    valid_scores = max_scores[mask]\n",
        "    valid_classes = class_ids[mask]\n",
        "    \n",
        "    if original_size is not None:\n",
        "        scale_x = original_size[0] / input_size\n",
        "        scale_y = original_size[1] / input_size\n",
        "        valid_boxes[:, [0, 2]] *= scale_x\n",
        "        valid_boxes[:, [1, 3]] *= scale_y\n",
        "    \n",
        "    keep_indices = nms(valid_boxes, valid_scores, iou_thres)\n",
        "    \n",
        "    return {\n",
        "        \"boxes\": valid_boxes[keep_indices].cpu().numpy().tolist(),\n",
        "        \"scores\": valid_scores[keep_indices].cpu().numpy().tolist(),\n",
        "        \"classes\": valid_classes[keep_indices].cpu().numpy().tolist(),\n",
        "        \"num_detections\": len(keep_indices)\n",
        "    }\n",
        "\n",
        "\n",
        "def draw_boxes(img: Image.Image, boxes: List, scores: List, classes: List):\n",
        "    \"\"\"Draw bounding boxes on image.\"\"\"\n",
        "    img_draw = img.copy()\n",
        "    draw = ImageDraw.Draw(img_draw)\n",
        "    \n",
        "    try:\n",
        "        font = ImageFont.truetype(\"/System/Library/Fonts/Helvetica.ttc\", 12)\n",
        "    except:\n",
        "        font = ImageFont.load_default()\n",
        "    \n",
        "    for box, score, cls_id in zip(boxes, scores, classes):\n",
        "        x1, y1, x2, y2 = box\n",
        "        cls_id_int = int(cls_id)\n",
        "        color = get_class_color(cls_id_int)\n",
        "        \n",
        "        draw.rectangle([x1, y1, x2, y2], outline=color, width=2)\n",
        "        \n",
        "        label = f\"{COCO_CLASSES[cls_id_int]}: {score:.2f}\" if cls_id_int < len(COCO_CLASSES) else f\"Class {cls_id_int}: {score:.2f}\"\n",
        "        bbox = draw.textbbox((0, 0), label, font=font)\n",
        "        text_width, text_height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
        "        \n",
        "        draw.rectangle([x1, y1 - text_height - 4, x1 + text_width + 4, y1], fill=color)\n",
        "        draw.text((x1 + 2, y1 - text_height - 2), label, fill=\"white\", font=font)\n",
        "    \n",
        "    return img_draw\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model and Run Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_SIZE = \"n\"  # Options: \"n\", \"s\", \"m\", \"l\", \"x\"\n",
        "WEIGHTS_PATH = \"../weights/libreyolo8n.pt\"\n",
        "IMAGE_PATH = \"../media/test_image_1_creative_commons.jpg\"\n",
        "\n",
        "# Initialize model\n",
        "model = LibreYOLO8Model(config=MODEL_SIZE, reg_max=16, nb_classes=80)\n",
        "\n",
        "# Load weights\n",
        "state_dict = torch.load(WEIGHTS_PATH, map_location='cpu', weights_only=False)\n",
        "model.load_state_dict(state_dict, strict=True)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess image\n",
        "input_tensor, original_img, original_size = preprocess_image(IMAGE_PATH, input_size=640)\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "\n",
        "# Postprocess\n",
        "detections = postprocess(output, conf_thres=0.25, iou_thres=0.45, input_size=640, original_size=original_size)\n",
        "\n",
        "print(f\"Found {detections['num_detections']} detections\")\n",
        "for i, (box, score, cls_id) in enumerate(zip(detections['boxes'], detections['scores'], detections['classes'])):\n",
        "    print(f\"  {i+1}. {COCO_CLASSES[int(cls_id)]}: {score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Draw detections and display\n",
        "if detections['num_detections'] > 0:\n",
        "    result_img = draw_boxes(original_img, detections['boxes'], detections['scores'], detections['classes'])\n",
        "else:\n",
        "    result_img = original_img\n",
        "\n",
        "# Display the result\n",
        "result_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Save the result\n",
        "output_path = \"../media/notebook_output.jpg\"\n",
        "result_img.save(output_path)\n",
        "print(f\"Result saved to: {output_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
