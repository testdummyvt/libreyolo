{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29348b89",
   "metadata": {},
   "source": [
    "# LibreYOLO11 - Minimal Inference Notebook\n",
    "\n",
    "This notebook contains all the code needed to run inference with a self-contained implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple, Dict, List\n",
    "import colorsys\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0557ba",
   "metadata": {},
   "source": [
    "## Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44caac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFL(nn.Module):\n",
    "    \"\"\"Distribution Focal Loss (DFL) module.\"\"\"\n",
    "    def __init__(self, c1=16):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)\n",
    "        x = torch.arange(c1, dtype=torch.float)\n",
    "        self.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1))\n",
    "        self.c1 = c1\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.view(b, 4, self.c1, h, w).transpose(2, 1)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        x = self.conv(x.reshape(b, self.c1, 4 * h, w))\n",
    "        return x.view(b, 4, h, w)\n",
    "\n",
    "\n",
    "def autopad(k, p=None, d=1):\n",
    "    \"\"\"Pad to 'same' output shape.\"\"\"\n",
    "    if d > 1:\n",
    "        if isinstance(k, int):\n",
    "            k = d * (k - 1) + 1\n",
    "        else:\n",
    "            k = tuple(d * (x - 1) + 1 for x in k)\n",
    "    if p is None:\n",
    "        if isinstance(k, int):\n",
    "            p = k // 2\n",
    "        else:\n",
    "            p = tuple(x // 2 for x in k)\n",
    "    return p\n",
    "\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    \"\"\"Standard convolution block: Conv2d + BatchNorm + SiLU\"\"\"\n",
    "    def __init__(self, c_out, c_in, k, s, p=None, g=1, d=1, act=True):\n",
    "        super().__init__()\n",
    "        if p is None:\n",
    "            p = autopad(k, p, d)\n",
    "        self.cnn = nn.Conv2d(in_channels=c_in, out_channels=c_out, kernel_size=k, stride=s, padding=p, groups=g, dilation=d, bias=False)\n",
    "        self.batchnorm = nn.BatchNorm2d(num_features=c_out, eps=0.001, momentum=0.03)\n",
    "        self.silu = nn.SiLU() if act else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.silu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DWConv(Conv):\n",
    "    \"\"\"Depthwise convolution\"\"\"\n",
    "    def __init__(self, c_out, c_in, k=3, s=1, d=1, act=True):\n",
    "        super().__init__(c_out, c_in, k=k, s=s, p=None, g=math.gcd(c_in, c_out), d=d, act=act)\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"Standard bottleneck block\"\"\"\n",
    "    def __init__(self, c_out, c_in, shortcut=True, g=1, k=(3, 3), e=0.5):\n",
    "        super().__init__()\n",
    "        c_ = int(c_out * e)\n",
    "        self.conv1 = Conv(c_out=c_, c_in=c_in, k=k[0], s=1, p=autopad(k[0]))\n",
    "        self.conv2 = Conv(c_out=c_out, c_in=c_, k=k[1], s=1, p=autopad(k[1]), g=g)\n",
    "        self.add = shortcut and c_in == c_out\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.conv2(self.conv1(x))\n",
    "        return x + y if self.add else y\n",
    "\n",
    "\n",
    "class C3(nn.Module):\n",
    "    \"\"\"CSP bottleneck with 3 convs (used by C3k).\"\"\"\n",
    "    def __init__(self, c_out, c_in, n=1, shortcut=True, g=1, e=0.5):\n",
    "        super().__init__()\n",
    "        c_ = int(c_out * e)\n",
    "        self.conv1 = Conv(c_out=c_, c_in=c_in, k=1, s=1, p=0)\n",
    "        self.conv2 = Conv(c_out=c_, c_in=c_in, k=1, s=1, p=0)\n",
    "        self.conv3 = Conv(c_out=c_out, c_in=2 * c_, k=1, s=1, p=0)\n",
    "        self.bottlenecks = nn.Sequential(*(Bottleneck(c_out=c_, c_in=c_, shortcut=shortcut, g=g, k=(1, 3), e=1.0) for _ in range(n)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv3(torch.cat((self.bottlenecks(self.conv1(x)), self.conv2(x)), dim=1))\n",
    "\n",
    "\n",
    "class C3k(C3):\n",
    "    \"\"\"C3 with customizable kernel-size in bottlenecks.\"\"\"\n",
    "    def __init__(self, c_out, c_in, n=1, shortcut=True, g=1, e=0.5, k=3):\n",
    "        super().__init__(c_out, c_in, n=n, shortcut=shortcut, g=g, e=e)\n",
    "        c_ = int(c_out * e)\n",
    "        self.bottlenecks = nn.Sequential(*(Bottleneck(c_out=c_, c_in=c_, shortcut=shortcut, g=g, k=(k, k), e=1.0) for _ in range(n)))\n",
    "\n",
    "\n",
    "class C3k2(nn.Module):\n",
    "    \"\"\"C2f-like block with optional C3k inner blocks.\"\"\"\n",
    "    def __init__(self, c_out, c_in, n=1, c3k=False, e=0.5, g=1, shortcut=True):\n",
    "        super().__init__()\n",
    "        self.c = int(c_out * e)\n",
    "        self.conv1 = Conv(c_out=2 * self.c, c_in=c_in, k=1, s=1, p=0)\n",
    "        self.conv2 = Conv(c_out=c_out, c_in=(2 + n) * self.c, k=1, s=1, p=0)\n",
    "        self.bottlenecks = nn.ModuleList(\n",
    "            C3k(c_out=self.c, c_in=self.c, n=2, shortcut=shortcut, g=g) if c3k \n",
    "            else Bottleneck(c_out=self.c, c_in=self.c, shortcut=shortcut, g=g) \n",
    "            for _ in range(n)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = list(self.conv1(x).chunk(2, dim=1))\n",
    "        y.extend(m(y[-1]) for m in self.bottlenecks)\n",
    "        return self.conv2(torch.cat(y, dim=1))\n",
    "\n",
    "\n",
    "class SPPF(nn.Module):\n",
    "    \"\"\"Spatial Pyramid Pooling Fast\"\"\"\n",
    "    def __init__(self, c_out, c_in, k=5):\n",
    "        super().__init__()\n",
    "        c_ = c_in // 2\n",
    "        self.conv1 = Conv(c_out=c_, c_in=c_in, k=1, s=1, p=0)\n",
    "        self.conv2 = Conv(c_out=c_out, c_in=c_ * 4, k=1, s=1, p=0)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        y1 = self.maxpool(x)\n",
    "        y2 = self.maxpool(y1)\n",
    "        y3 = self.maxpool(y2)\n",
    "        return self.conv2(torch.cat((x, y1, y2, y3), dim=1))\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Ultralytics-style attention (used by C2PSA).\"\"\"\n",
    "    def __init__(self, dim, num_heads=8, attn_ratio=0.5):\n",
    "        super().__init__()\n",
    "        if num_heads < 1:\n",
    "            raise ValueError(\"num_heads must be >= 1\")\n",
    "        if dim % num_heads != 0:\n",
    "            raise ValueError(f\"dim ({dim}) must be divisible by num_heads ({num_heads})\")\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.key_dim = int(self.head_dim * attn_ratio)\n",
    "        self.scale = self.key_dim ** -0.5\n",
    "\n",
    "        nh_kd = self.key_dim * num_heads\n",
    "        h = dim + nh_kd * 2\n",
    "        self.qkv = Conv(c_out=h, c_in=dim, k=1, s=1, p=0, act=False)\n",
    "        self.proj = Conv(c_out=dim, c_in=dim, k=1, s=1, p=0, act=False)\n",
    "        self.pe = Conv(c_out=dim, c_in=dim, k=3, s=1, p=1, g=dim, act=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        n = h * w\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.view(b, self.num_heads, self.key_dim * 2 + self.head_dim, n).split(\n",
    "            [self.key_dim, self.key_dim, self.head_dim], dim=2\n",
    "        )\n",
    "        attn = (q.transpose(-2, -1) @ k) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (v @ attn.transpose(-2, -1)).view(b, c, h, w) + self.pe(v.reshape(b, c, h, w))\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "class PSABlock(nn.Module):\n",
    "    \"\"\"PSA Block with Attention and FFN\"\"\"\n",
    "    def __init__(self, c, attn_ratio=0.5, num_heads=4, shortcut=True):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(c, attn_ratio=attn_ratio, num_heads=num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            Conv(c_out=c * 2, c_in=c, k=1, s=1, p=0),\n",
    "            Conv(c_out=c, c_in=c * 2, k=1, s=1, p=0, act=False)\n",
    "        )\n",
    "        self.add = shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(x) if self.add else self.attn(x)\n",
    "        x = x + self.ffn(x) if self.add else self.ffn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class C2PSA(nn.Module):\n",
    "    \"\"\"C2 + PSA (YOLO11).\"\"\"\n",
    "    def __init__(self, c_out, c_in, n=1, e=0.5):\n",
    "        super().__init__()\n",
    "        if c_in != c_out:\n",
    "            raise ValueError(\"C2PSA requires c_in == c_out\")\n",
    "        self.c = int(c_in * e)\n",
    "        self.conv1 = Conv(c_out=2 * self.c, c_in=c_in, k=1, s=1, p=0)\n",
    "        self.conv2 = Conv(c_out=c_in, c_in=2 * self.c, k=1, s=1, p=0)\n",
    "        nh = max(1, self.c // 64)\n",
    "        self.bottlenecks = nn.Sequential(*(PSABlock(self.c, attn_ratio=0.5, num_heads=nh) for _ in range(n)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        a, b = self.conv1(x).chunk(2, dim=1)\n",
    "        b = self.bottlenecks(b)\n",
    "        return self.conv2(torch.cat((a, b), dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c791801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    \"\"\"Feature extraction backbone for YOLO11\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.configuration = {\n",
    "            'n': {'d': 0.33, 'w': 0.25, 'r': 2.0},\n",
    "            's': {'d': 0.33, 'w': 0.50, 'r': 2.0},\n",
    "            'm': {'d': 0.67, 'w': 0.75, 'r': 1.5},\n",
    "            'l': {'d': 1.00, 'w': 1.00, 'r': 1.0},\n",
    "            'x': {'d': 1.00, 'w': 1.25, 'r': 1.0}\n",
    "        }\n",
    "\n",
    "        d = self.configuration[config]['d']\n",
    "        w = self.configuration[config]['w']\n",
    "        r = self.configuration[config]['r']\n",
    "        \n",
    "        # YOLO11n structure\n",
    "        self.p1 = Conv(c_out=16, c_in=3, k=3, s=2, p=1)\n",
    "        self.p2 = Conv(c_out=32, c_in=16, k=3, s=2, p=1)\n",
    "        self.c2f1 = C3k2(c_out=64, c_in=32, n=1, c3k=False, e=0.25, shortcut=True)\n",
    "        self.p3 = Conv(c_out=64, c_in=64, k=3, s=2, p=1)\n",
    "        self.c2f2 = C3k2(c_out=128, c_in=64, n=1, c3k=False, e=0.25, shortcut=True)\n",
    "        self.p4 = Conv(c_out=128, c_in=128, k=3, s=2, p=1)\n",
    "        self.c2f3 = C3k2(c_out=128, c_in=128, n=1, c3k=True, e=0.5, shortcut=True)\n",
    "        self.p5 = Conv(c_out=256, c_in=128, k=3, s=2, p=1)\n",
    "        self.c2f4 = C3k2(c_out=256, c_in=256, n=1, c3k=True, e=0.5, shortcut=True)\n",
    "        self.sppf = SPPF(c_out=256, c_in=256, k=5)\n",
    "        self.c2psa = C2PSA(c_out=256, c_in=256, n=1, e=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.p1(x)\n",
    "        x = self.p2(x)\n",
    "        x = self.c2f1(x)\n",
    "        x = self.p3(x)\n",
    "        x = self.c2f2(x)\n",
    "        x8 = x.clone()\n",
    "        \n",
    "        x = self.p4(x)\n",
    "        x = self.c2f3(x)\n",
    "        x16 = x.clone()\n",
    "        \n",
    "        x = self.p5(x)\n",
    "        x = self.c2f4(x)\n",
    "        x = self.sppf(x)\n",
    "        x32 = self.c2psa(x)\n",
    "        \n",
    "        return x8, x16, x32\n",
    "\n",
    "\n",
    "class Neck(nn.Module):\n",
    "    \"\"\"Feature pyramid network neck for YOLO11\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.configuration = {\n",
    "            'n': {'d': 0.33, 'w': 0.25, 'r': 2.0},\n",
    "            's': {'d': 0.33, 'w': 0.50, 'r': 2.0},\n",
    "            'm': {'d': 0.67, 'w': 0.75, 'r': 1.5},\n",
    "            'l': {'d': 1.00, 'w': 1.00, 'r': 1.0},\n",
    "            'x': {'d': 1.00, 'w': 1.25, 'r': 1.0}\n",
    "        }\n",
    "\n",
    "        d = self.configuration[config]['d']\n",
    "        w = self.configuration[config]['w']\n",
    "        r = self.configuration[config]['r']\n",
    "\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.c2f21 = C3k2(c_out=128, c_in=384, n=1, c3k=False, e=0.5, shortcut=True)\n",
    "        \n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.c2f11 = C3k2(c_out=64, c_in=256, n=1, c3k=False, e=0.5, shortcut=True)\n",
    "        \n",
    "        self.conv1 = Conv(c_out=64, c_in=64, k=3, s=2, p=1)\n",
    "        self.c2f12 = C3k2(c_out=128, c_in=192, n=1, c3k=False, e=0.5, shortcut=True)\n",
    "        \n",
    "        self.conv2 = Conv(c_out=128, c_in=128, k=3, s=2, p=1)\n",
    "        self.c2f22 = C3k2(c_out=256, c_in=384, n=1, c3k=True, e=0.5, shortcut=True)\n",
    "        \n",
    "    def forward(self, x8, x16, x32):\n",
    "        aux1 = self.upsample1(x32)\n",
    "        x16 = torch.cat((aux1, x16), dim=1)\n",
    "        x16 = self.c2f21(x16)\n",
    "        \n",
    "        aux2 = self.upsample2(x16)\n",
    "        x8 = torch.cat((aux2, x8), dim=1)\n",
    "        x8 = self.c2f11(x8)\n",
    "        \n",
    "        aux3 = self.conv1(x8)\n",
    "        x16 = torch.cat((aux3, x16), dim=1)\n",
    "        x16 = self.c2f12(x16)\n",
    "        \n",
    "        aux4 = self.conv2(x16)\n",
    "        x32 = torch.cat((aux4, x32), dim=1)\n",
    "        x32 = self.c2f22(x32)\n",
    "        \n",
    "        return x8, x16, x32\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"Decoupled detection head for YOLO11\"\"\"\n",
    "    def __init__(self, c_in, c_box, c_cls, reg_max, nb_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv11 = Conv(c_out=c_box, c_in=c_in, k=3, s=1, p=1)\n",
    "        self.conv12 = Conv(c_out=c_box, c_in=c_box, k=3, s=1, p=1)\n",
    "        self.cnn1 = nn.Conv2d(in_channels=c_box, out_channels=4*reg_max, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        self.conv21 = nn.Sequential(\n",
    "            DWConv(c_out=c_in, c_in=c_in, k=3, s=1),\n",
    "            Conv(c_out=c_cls, c_in=c_in, k=1, s=1, p=0)\n",
    "        )\n",
    "        self.conv22 = nn.Sequential(\n",
    "            DWConv(c_out=c_cls, c_in=c_cls, k=3, s=1),\n",
    "            Conv(c_out=c_cls, c_in=c_cls, k=1, s=1, p=0, act=True)\n",
    "        )\n",
    "        self.cnn2 = nn.Conv2d(in_channels=c_cls, out_channels=nb_classes, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        box = self.cnn1(self.conv12(self.conv11(x)))\n",
    "        cls = self.cnn2(self.conv22(self.conv21(x)))\n",
    "        return box, cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0494d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibreYOLO11Model(nn.Module):\n",
    "    \"\"\"Main YOLO11 model\"\"\"\n",
    "    def __init__(self, config, reg_max=16, nb_classes=80):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.configuration = {\n",
    "            'n': {'d': 0.33, 'w': 0.25, 'r': 2.0},\n",
    "            's': {'d': 0.33, 'w': 0.50, 'r': 2.0},\n",
    "            'm': {'d': 0.67, 'w': 0.75, 'r': 1.5},\n",
    "            'l': {'d': 1.00, 'w': 1.00, 'r': 1.0},\n",
    "            'x': {'d': 1.00, 'w': 1.25, 'r': 1.0}\n",
    "        }\n",
    "\n",
    "        d = self.configuration[config]['d']\n",
    "        w = self.configuration[config]['w']\n",
    "        r = self.configuration[config]['r']\n",
    "        \n",
    "        self.backbone = Backbone(config=config)\n",
    "        self.neck = Neck(config=config)\n",
    "        \n",
    "        c_box = max(16, 64 // 4, reg_max * 4)\n",
    "        c_cls = max(64, min(nb_classes, 100))\n",
    "        \n",
    "        self.head8 = Head(c_in=64, c_box=c_box, c_cls=c_cls, reg_max=reg_max, nb_classes=nb_classes)\n",
    "        self.head16 = Head(c_in=128, c_box=c_box, c_cls=c_cls, reg_max=reg_max, nb_classes=nb_classes)\n",
    "        self.head32 = Head(c_in=256, c_box=c_box, c_cls=c_cls, reg_max=reg_max, nb_classes=nb_classes)\n",
    "        \n",
    "        self.dfl = DFL(c1=reg_max)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x8, x16, x32 = self.backbone(x)\n",
    "        x8, x16, x32 = self.neck(x8, x16, x32)\n",
    "        \n",
    "        box8, cls8 = self.head8(x8)\n",
    "        box16, cls16 = self.head16(x16)\n",
    "        box32, cls32 = self.head32(x32)\n",
    "        \n",
    "        decoded_box8 = self.dfl(box8)\n",
    "        decoded_box16 = self.dfl(box16)\n",
    "        decoded_box32 = self.dfl(box32)\n",
    "        \n",
    "        return {\n",
    "            'x8': {'box': decoded_box8, 'cls': cls8},\n",
    "            'x16': {'box': decoded_box16, 'cls': cls16},\n",
    "            'x32': {'box': decoded_box32, 'cls': cls32}\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bfcaa8",
   "metadata": {},
   "source": [
    "## Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1c2293",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_CLASSES = [\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',\n",
    "    'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',\n",
    "    'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',\n",
    "    'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',\n",
    "    'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "\n",
    "def get_class_color(class_id: int) -> str:\n",
    "    \"\"\"Get a unique color for a class ID.\"\"\"\n",
    "    hue = (class_id * 137.508) % 360 / 360.0\n",
    "    saturation = 0.7 + (class_id % 3) * 0.1\n",
    "    value = 0.8 + (class_id % 2) * 0.15\n",
    "    rgb = colorsys.hsv_to_rgb(hue, saturation, value)\n",
    "    return f\"#{int(rgb[0]*255):02x}{int(rgb[1]*255):02x}{int(rgb[2]*255):02x}\"\n",
    "\n",
    "\n",
    "def preprocess_image(image: Union[str, Image.Image, np.ndarray], input_size: int = 640):\n",
    "    \"\"\"Preprocess image for model inference.\"\"\"\n",
    "    if isinstance(image, str):\n",
    "        img = Image.open(image).convert('RGB')\n",
    "    elif isinstance(image, Image.Image):\n",
    "        img = image.convert('RGB')\n",
    "    elif isinstance(image, np.ndarray):\n",
    "        img = Image.fromarray(image).convert('RGB')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported image type: {type(image)}\")\n",
    "    \n",
    "    original_size = img.size\n",
    "    original_img = img.copy()\n",
    "    img_resized = img.resize((input_size, input_size), Image.Resampling.BILINEAR)\n",
    "    img_array = np.array(img_resized, dtype=np.float32) / 255.0\n",
    "    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "    return img_tensor, original_img, original_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f1d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_anchors(feats: List[torch.Tensor], strides: List[int], grid_cell_offset: float = 0.5):\n",
    "    \"\"\"Generate anchor points from feature map sizes.\"\"\"\n",
    "    anchor_points, stride_tensor = [], []\n",
    "    \n",
    "    for feat, stride in zip(feats, strides):\n",
    "        _, _, h, w = feat.shape\n",
    "        dtype, device = feat.dtype, feat.device\n",
    "        sx = torch.arange(end=w, device=device, dtype=dtype) + grid_cell_offset\n",
    "        sy = torch.arange(end=h, device=device, dtype=dtype) + grid_cell_offset\n",
    "        sy, sx = torch.meshgrid(sy, sx, indexing='ij')\n",
    "        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
    "        stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n",
    "    \n",
    "    return torch.cat(anchor_points), torch.cat(stride_tensor)\n",
    "\n",
    "\n",
    "def decode_boxes(box_preds: torch.Tensor, anchors: torch.Tensor, stride_tensor: torch.Tensor):\n",
    "    \"\"\"Decode box predictions to xyxy coordinates.\"\"\"\n",
    "    x1 = (anchors[:, 0:1] - box_preds[0, :, 0:1]) * stride_tensor[:, 0:1]\n",
    "    y1 = (anchors[:, 1:2] - box_preds[0, :, 1:2]) * stride_tensor[:, 0:1]\n",
    "    x2 = (anchors[:, 0:1] + box_preds[0, :, 2:3]) * stride_tensor[:, 0:1]\n",
    "    y2 = (anchors[:, 1:2] + box_preds[0, :, 3:4]) * stride_tensor[:, 0:1]\n",
    "    return torch.cat([x1, y1, x2, y2], dim=1)\n",
    "\n",
    "\n",
    "def nms(boxes: torch.Tensor, scores: torch.Tensor, iou_threshold: float = 0.45):\n",
    "    \"\"\"Non-Maximum Suppression.\"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return torch.tensor([], dtype=torch.long, device=boxes.device)\n",
    "    \n",
    "    _, order = scores.sort(0, descending=True)\n",
    "    keep = []\n",
    "    \n",
    "    while len(order) > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i.item())\n",
    "        if len(order) == 1:\n",
    "            break\n",
    "        \n",
    "        box_i = boxes[i]\n",
    "        boxes_remaining = boxes[order[1:]]\n",
    "        \n",
    "        x1_i, y1_i, x2_i, y2_i = box_i\n",
    "        x1_r, y1_r, x2_r, y2_r = boxes_remaining[:, 0], boxes_remaining[:, 1], boxes_remaining[:, 2], boxes_remaining[:, 3]\n",
    "        \n",
    "        x1_inter = torch.max(x1_i, x1_r)\n",
    "        y1_inter = torch.max(y1_i, y1_r)\n",
    "        x2_inter = torch.min(x2_i, x2_r)\n",
    "        y2_inter = torch.min(y2_i, y2_r)\n",
    "        \n",
    "        inter_area = torch.clamp(x2_inter - x1_inter, min=0) * torch.clamp(y2_inter - y1_inter, min=0)\n",
    "        area_i = (x2_i - x1_i) * (y2_i - y1_i)\n",
    "        area_r = (x2_r - x1_r) * (y2_r - y1_r)\n",
    "        iou = inter_area / (area_i + area_r - inter_area + 1e-7)\n",
    "        \n",
    "        order = order[1:][iou < iou_threshold]\n",
    "    \n",
    "    return torch.tensor(keep, dtype=torch.long, device=boxes.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b490b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(output: Dict, conf_thres: float = 0.25, iou_thres: float = 0.45, \n",
    "                input_size: int = 640, original_size: Tuple[int, int] = None):\n",
    "    \"\"\"Postprocess model outputs to get final detections.\"\"\"\n",
    "    box_layers = [output['x8']['box'], output['x16']['box'], output['x32']['box']]\n",
    "    cls_layers = [output['x8']['cls'], output['x16']['cls'], output['x32']['cls']]\n",
    "    strides = [8, 16, 32]\n",
    "    \n",
    "    anchors, stride_tensor = make_anchors(box_layers, strides)\n",
    "    box_preds = torch.cat([x.flatten(2).permute(0, 2, 1) for x in box_layers], dim=1)\n",
    "    cls_preds = torch.cat([x.flatten(2).permute(0, 2, 1) for x in cls_layers], dim=1)\n",
    "    \n",
    "    decoded_boxes = decode_boxes(box_preds, anchors, stride_tensor)\n",
    "    scores = cls_preds[0].sigmoid()\n",
    "    max_scores, class_ids = torch.max(scores, dim=1)\n",
    "    \n",
    "    mask = max_scores > conf_thres\n",
    "    if not mask.any():\n",
    "        return {\"boxes\": [], \"scores\": [], \"classes\": [], \"num_detections\": 0}\n",
    "    \n",
    "    valid_boxes = decoded_boxes[mask]\n",
    "    valid_scores = max_scores[mask]\n",
    "    valid_classes = class_ids[mask]\n",
    "    \n",
    "    if original_size is not None:\n",
    "        scale_x = original_size[0] / input_size\n",
    "        scale_y = original_size[1] / input_size\n",
    "        valid_boxes[:, [0, 2]] *= scale_x\n",
    "        valid_boxes[:, [1, 3]] *= scale_y\n",
    "    \n",
    "    keep_indices = nms(valid_boxes, valid_scores, iou_thres)\n",
    "    \n",
    "    return {\n",
    "        \"boxes\": valid_boxes[keep_indices].cpu().numpy().tolist(),\n",
    "        \"scores\": valid_scores[keep_indices].cpu().numpy().tolist(),\n",
    "        \"classes\": valid_classes[keep_indices].cpu().numpy().tolist(),\n",
    "        \"num_detections\": len(keep_indices)\n",
    "    }\n",
    "\n",
    "\n",
    "def draw_boxes(img: Image.Image, boxes: List, scores: List, classes: List):\n",
    "    \"\"\"Draw bounding boxes on image.\"\"\"\n",
    "    img_draw = img.copy()\n",
    "    draw = ImageDraw.Draw(img_draw)\n",
    "    \n",
    "    try:\n",
    "        font = ImageFont.truetype(\"/System/Library/Fonts/Helvetica.ttc\", 12)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    for box, score, cls_id in zip(boxes, scores, classes):\n",
    "        x1, y1, x2, y2 = box\n",
    "        cls_id_int = int(cls_id)\n",
    "        color = get_class_color(cls_id_int)\n",
    "        \n",
    "        draw.rectangle([x1, y1, x2, y2], outline=color, width=2)\n",
    "        \n",
    "        label = f\"{COCO_CLASSES[cls_id_int]}: {score:.2f}\" if cls_id_int < len(COCO_CLASSES) else f\"Class {cls_id_int}: {score:.2f}\"\n",
    "        bbox = draw.textbbox((0, 0), label, font=font)\n",
    "        text_width, text_height = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "        \n",
    "        draw.rectangle([x1, y1 - text_height - 4, x1 + text_width + 4, y1], fill=color)\n",
    "        draw.text((x1 + 2, y1 - text_height - 2), label, fill=\"white\", font=font)\n",
    "    \n",
    "    return img_draw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898773b8",
   "metadata": {},
   "source": [
    "## Load Model and Run Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f27fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_SIZE = \"n\"  # Options: \"n\", \"s\", \"m\", \"l\", \"x\"\n",
    "WEIGHTS_PATH = \"../temporary/libreyolo11n.pt\"\n",
    "IMAGE_PATH = \"../media/test_image_1_creative_commons.jpg\"\n",
    "\n",
    "# Initialize model\n",
    "model = LibreYOLO11Model(config=MODEL_SIZE, reg_max=16, nb_classes=80)\n",
    "\n",
    "# Load weights\n",
    "state_dict = torch.load(WEIGHTS_PATH, map_location='cpu', weights_only=False)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess image\n",
    "input_tensor, original_img, original_size = preprocess_image(IMAGE_PATH, input_size=640)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "# Postprocess\n",
    "detections = postprocess(output, conf_thres=0.25, iou_thres=0.45, input_size=640, original_size=original_size)\n",
    "\n",
    "print(f\"Found {detections['num_detections']} detections\")\n",
    "for i, (box, score, cls_id) in enumerate(zip(detections['boxes'], detections['scores'], detections['classes'])):\n",
    "    print(f\"  {i+1}. {COCO_CLASSES[int(cls_id)]}: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db9141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw detections and display\n",
    "if detections['num_detections'] > 0:\n",
    "    result_img = draw_boxes(original_img, detections['boxes'], detections['scores'], detections['classes'])\n",
    "else:\n",
    "    result_img = original_img\n",
    "\n",
    "# Display the result\n",
    "result_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256aea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save the result\n",
    "output_path = \"../media/notebook_output_yolo11.jpg\"\n",
    "result_img.save(output_path)\n",
    "print(f\"Result saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
