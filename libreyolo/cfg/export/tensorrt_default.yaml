# TensorRT Export Configuration
# ==============================
# This file defines the default settings for TensorRT engine export.
# Copy and modify this file for your specific deployment needs.

# Precision mode
# Options: fp32, fp16, int8
# - fp32: Full precision, maximum accuracy, largest engine
# - fp16: Half precision, recommended balance of speed/accuracy (default)
# - int8: Quantized, fastest but requires calibration data
precision: fp16

# GPU workspace size in GiB
# Larger values may find faster kernels but use more GPU memory during build.
# Does NOT affect inference memory usage.
# Recommended: 4-8 GiB for most models
workspace: 4.0

# Verbose logging during build
# Set to true for debugging export issues
verbose: false

# Hardware compatibility level
# Options:
# - none: Optimize for current GPU only (fastest, not portable)
# - ampere_plus: Works on Ampere (RTX 30xx, A100) and newer GPUs
# - same_compute_capability: Works on GPUs with same SM version
#
# Note: "none" produces the fastest engine but only works on the exact GPU
# it was built on. Use "ampere_plus" for deployment across multiple GPU types.
hardware_compatibility: none

# GPU device ID for multi-GPU systems
# Set to the GPU index (0, 1, 2, ...) to use for building
# Default: 0 (first GPU)
device: 0

# Dynamic batching configuration
# Enable this if you need to run inference with varying batch sizes
dynamic:
  enabled: false
  min_batch: 1
  opt_batch: 1   # TensorRT optimizes kernels for this batch size
  max_batch: 8

# INT8 calibration settings (only used when precision=int8)
# INT8 quantization requires a representative calibration dataset
# to compute optimal quantization scales for each layer.
int8_calibration:
  # Calibration dataset (YAML path)
  # Use a dataset representative of your inference data
  # Recommended: 500-1000 images for good accuracy
  dataset: coco5000.yaml

  # Fraction of dataset to use for calibration
  # 0.1 = 10% of dataset (e.g., 500 images from coco5000)
  fraction: 0.1

  # Cache calibration results for faster rebuilds
  # The cache file will be saved alongside the engine
  cache: true

# Output settings
output:
  # Add precision suffix to filename (e.g., model_fp16.engine)
  add_precision_suffix: true

  # Overwrite existing engine file
  overwrite: true
